{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b57ded0-eea2-4e70-ad93-e37b34c44331",
   "metadata": {},
   "source": [
    "### Data Gathering and Import\n",
    "\n",
    "First we use pandas to read from the dataset into our data frame.\n",
    "\n",
    "We'll also display the first few rows so we get an idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0316d5d4-6e54-49b0-9f74-5b265df80756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d6b51-dd70-45d9-b31f-7a5eb67d87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('financial_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf3cde-af9e-4517-9e23-b9153a694107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0312143-8610-4ade-8ad2-bdee67b16804",
   "metadata": {},
   "source": [
    "### Data Cleanup and Normalization\n",
    "\n",
    "##### Data Cleanup\n",
    "Many rows contain null values or NaN values. We will:\n",
    "\n",
    "- Locate the missing values\n",
    "- Replace missing values with the median for that specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870ce40-02c7-4577-b575-3c601e6dea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values per column\n",
    "missing_vals = df.isnull().sum()\n",
    "print(missing_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64fe3d-81ec-43f7-8b8d-daee78342081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the columns that have null values\n",
    "cols_missing_vals = ['Income', 'Credit Score', 'Loan Amount', 'Assets Value', 'Number of Dependents', 'Previous Defaults']\n",
    "\n",
    "# figure out the medians for those columns\n",
    "medians = df[cols_missing_vals].median()\n",
    "\n",
    "# reassign null values within those columns to their respective medians\n",
    "df[cols_missing_vals] = df[cols_missing_vals].fillna(medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f352c-5e6b-4b99-a16d-3254fe82d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if what we did worked\n",
    "missing_vals_per_col = df.isnull().sum()\n",
    "\n",
    "total_missing_vals = missing_vals_per_col.sum()\n",
    "\n",
    "print(missing_vals_per_col)\n",
    "print(\"\\nTotal missing values in the entire dataset: \", total_missing_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03cd21a-9642-42ba-a925-b0a44f9cba73",
   "metadata": {},
   "source": [
    "##### Data Normalization\n",
    "\n",
    "We normalize the columns containing numbers to be in the range $[0, 1]$. This will be helpful if we decisde to use KNN, SVM, boosting or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4d4f5-4d65-4b2c-bd06-f3442fbc1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# make a copy of the dataset to normalize it\n",
    "df_norm = df.copy()\n",
    "\n",
    "# extract the columns containing numbers\n",
    "number_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_norm[number_cols] = scaler.fit_transform(df_norm[number_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfb113-009d-4903-9590-866a77e1c65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f55bf-5ea0-4657-a4cf-55aa6a797daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of location information, there are too many categories here\n",
    "df = df.drop(columns=['City', 'State', 'Country'])\n",
    "df_norm = df_norm.drop(columns=['City', 'State', 'Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07fc2d-d60d-432a-b0dd-48e12747dde7",
   "metadata": {},
   "source": [
    "### Data Transformation and Conversion\n",
    "\n",
    "We must now convert columns containing categories of data into numerical values. \n",
    "\n",
    "We will use OneHotEncoder to encode categorical values into binary, then transform the columns to prevent redundancy using ColumnTransformer. \n",
    "\n",
    "For example, the Education Level column will split into \"Education Level_High School\" and \"Education Level_Master's\" and \"Education Level_PhD\". Each will be assigned a 0 or 1. If all columns are 0, the Education Level must be 'Bachelor's'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0712c-508e-4623-b090-687fdcb34aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# find the columns that are not numbers (categories)\n",
    "category_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# encode the data, use drop='first' to avoid dummy variables and prevent redundancy\n",
    "category_encoder = ColumnTransformer(\n",
    "    transformers = [('Category Column Encoder', OneHotEncoder(drop='first', sparse_output=False), category_cols)], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# apply the encode to the normalized and non-normalized data\n",
    "df_encoded = category_encoder.fit_transform(df)\n",
    "df_norm_encoded = category_encoder.fit_transform(df_norm)\n",
    "\n",
    "\n",
    "# create new column names based on categories\n",
    "new_col_names = (\n",
    "    category_encoder.named_transformers_['Category Column Encoder'].get_feature_names_out(category_cols).tolist() + number_cols.tolist()\n",
    ")\n",
    "\n",
    "# transform the encoded data back into a data frame using the column names\n",
    "df_transformed = pd.DataFrame(df_encoded, columns=new_col_names)\n",
    "df_norm_transformed = pd.DataFrame(df_norm_encoded, columns=new_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124da6c-5c2d-4e0a-a938-7dc21ce35f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to reconstruct the 'Risk Rating' column since it was split up in the previous step\n",
    "risk_rating_cols1 = [col for col in df_transformed.columns if col.startswith('Risk Rating')]\n",
    "risk_rating_cols2 = [col for col in df_norm_transformed.columns if col.startswith('Risk Rating')]\n",
    "\n",
    "df_transformed['Risk Rating'] = np.argmax(df_transformed[risk_rating_cols1].values, axis=1)\n",
    "df_norm_transformed['Risk Rating'] = np.argmax(df_norm_transformed[risk_rating_cols2].values, axis=1)\n",
    "\n",
    "rating_categories = ['Low', 'Medium', 'High']\n",
    "df_transformed['Risk Rating'] = df_transformed['Risk Rating'].apply(lambda rating: rating_categories[rating])\n",
    "df_norm_transformed['Risk Rating'] = df_norm_transformed['Risk Rating'].apply(lambda rating: rating_categories[rating])\n",
    "\n",
    "df_transformed = df_transformed.drop(columns=risk_rating_cols1)\n",
    "df_norm_transformed = df_norm_transformed.drop(columns=risk_rating_cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7f6b0-ef12-44a8-ad7f-1ab67cd785d1",
   "metadata": {},
   "source": [
    "Data frame after transformation and conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5be02-7b32-4fe7-baae-dc1c78bce6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfe4d2-4c9e-42e7-84ea-7f51fb47ee5c",
   "metadata": {},
   "source": [
    "Normalized data frame after transformation and conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1c918-64c7-4fc5-b99d-07a059de8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea66ae7-3090-4bc5-aafe-583b5a936609",
   "metadata": {},
   "source": [
    "### Machine Learning Setup\n",
    "\n",
    "Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb623c46-8b68-4277-a25f-ed239429f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66a876-e8e6-46bc-a6e9-dd4bfb43c8b4",
   "metadata": {},
   "source": [
    "Test allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aec35-5221-4f54-864e-0e26b43280ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_transformed.drop(columns=['Risk Rating'])\n",
    "Y = df_transformed['Risk Rating']\n",
    "\n",
    "Xn = df_norm_transformed.drop(columns=['Risk Rating'])\n",
    "Yn = df_norm_transformed['Risk Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4ac65-23db-4b16-a4ab-5e3eadaec774",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=100)\n",
    "\n",
    "Xn_train, Xn_test, Yn_train, Yn_test = train_test_split(Xn, Yn, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb799a-29c6-46c6-98ab-4e40995249f8",
   "metadata": {},
   "source": [
    "##### Machine Learning Algorithms\n",
    "\n",
    "First defining all the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7c640-d285-476d-a4f7-be2f5f281cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forcast = RandomForestClassifier()\n",
    "\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f461119-6025-45f1-857e-738320dd1a09",
   "metadata": {},
   "source": [
    "##### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2487a1e-0e1c-437d-b24d-3a22d24de730",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forcast.fit(X_train, Y_train)\n",
    "\n",
    "random_forcast_prediction = random_forcast.predict(X_test)\n",
    "\n",
    "random_forest_accuracy = accuracy_score(Y_test, random_forcast_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51deb520-a766-4b79-bc1d-9b2cd04c6812",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bdee4-0212-4d86-9ec9-46d1de7adccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting.fit(Xn_train, Yn_train)\n",
    "\n",
    "gradient_boosting_prediction = gradient_boosting.predict(Xn_test)\n",
    "\n",
    "gradient_boosting_accuracy = accuracy_score(Yn_test, gradient_boosting_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f28a58-dfa1-4c96-b8a8-ba1625323d9f",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b4b29-e63e-4294-b3bc-69ef8a93d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes.fit(X_train, Y_train)\n",
    "\n",
    "naive_bayes.score(X_train, Y_train)\n",
    "\n",
    "naive_bayes_prediction = naive_bayes.predict(X_test)\n",
    "\n",
    "naive_bayes_accuracy = accuracy_score(Y_test, naive_bayes_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5c1f7-6d66-48d2-8862-9ba7b33d5a59",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9834d0c-5690-4ae0-8b56-39f5529b2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN.fit(Xn_train, Yn_train)\n",
    "\n",
    "KNN_prediction = KNN.predict(Xn_test)\n",
    "\n",
    "KNN_accuracy = accuracy_score(Yn_test, KNN_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc4eb2-da33-4a6b-a596-4bfc20d5a6bc",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fcf77-4e59-409d-876d-b3deebee4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forests Accuracy: \", random_forest_accuracy * 100, \"%\")\n",
    "print(\"Gradient Boosting Accuracy: \", gradient_boosting_accuracy * 100, \"%\")\n",
    "print(\"Naive Bayes Accuracy: \", naive_bayes_accuracy * 100, \"%\")\n",
    "print(\"KNN Accuracy: \", KNN_accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d30f9-6212-4fb0-a52e-30ab81df4b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
